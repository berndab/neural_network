{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from   tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.read_csv(\"charity_data.preprocess.2.one_hot_encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = model_df.IS_SUCCESSFUL.count()\n",
    "print(f\"The number of rows in the data set is: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The neural network model is designed to \n",
    "# test the model by setting a threshold\n",
    "# to remove ASK_AMT outlier values\n",
    "\n",
    "feature_ask_amt_outlier_le_threshold = 100000000000000\n",
    "\n",
    "\n",
    "# The neural network model is designed to \n",
    "# test the model with various binning \n",
    "# thresholds for the features\n",
    "# \n",
    "# APPLICATION_TYPE\n",
    "# CLASSIFICATION\n",
    "\n",
    "feature_appl_binning_le_row_cnt = 10\n",
    "\n",
    "feature_class_binning_le_row_cnt = 4\n",
    "\n",
    "hidden_layer_1_nodes   = 75\n",
    "hidden_layer_1_act_func = \"relu\"\n",
    "\n",
    "hidden_layer_2_nodes   = 150\n",
    "hidden_layer_2_act_func = \"relu\"\n",
    "\n",
    "hidden_layer_3_nodes   = 10\n",
    "hidden_layer_3_act_func = \"relu\"\n",
    "\n",
    "output_layer_nodes = 1\n",
    "output_layer_act_func = \"sigmoid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove target ASK_AMT Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = model_df[(model_df[\"ASK_AMT\"] <= feature_ask_amt_outlier_le_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count_removed = row_count - model_df.IS_SUCCESSFUL.count()\n",
    "print(f\"Eliminating ASK_AMT outliers over {feature_ask_amt_outlier_le_threshold} dollars removed {row_count_removed} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bin features APPLICATION_TYPE and CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remapp(classification, remap_class_list):\n",
    "    if classification in remap_class_list:\n",
    "        return \"Other\"\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_feature_binning(feature_Name, binning_le_row_cnt):\n",
    "    ## if binning row count threshold is > 0 bin feature\n",
    "    if binning_le_row_cnt > 0:\n",
    "        # Get feature unique classification row counts\n",
    "        feature_cat_filt_row_cnt_ser = model_df[feature_Name].value_counts()\n",
    "        \n",
    "        # Filter classification values that have a row counts that\n",
    "        # are less than or equal to the binning threshold row count\n",
    "        feature_cat_filt_row_cnt_ser = feature_cat_filt_row_cnt_ser[(feature_cat_filt_row_cnt_ser[:] <= binning_le_row_cnt)]\n",
    "        \n",
    "        # Set feature catigory value to other for feature \n",
    "        # catigory values that have been binned\n",
    "        model_df[feature_Name] = model_df[feature_Name].apply(remapp, args=[feature_cat_filt_row_cnt_ser.index])\n",
    "        \n",
    "        ## get \n",
    "        feature_updated_cat_count = model_df[feature_Name].nunique()\n",
    "        print(f\" This feature {feature_Name} has the binning row count threshold of {binning_le_row_cnt} and has reduced that catigory values to {feature_updated_cat_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_count = model_df[\"APPLICATION_TYPE\"].nunique()\n",
    "print(f\"The number of unique catigory values for the feature APPLICATION_TYPE is {unique_class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_count = model_df[\"CLASSIFICATION\"].nunique()\n",
    "print(f\"The number of unique catigory values for the feature CLASSIFICATION is {unique_class_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_feature_binning(\"APPLICATION_TYPE\", feature_appl_binning_le_row_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_feature_binning(\"CLASSIFICATION\", feature_class_binning_le_row_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Catigorical Fields APPLICATION_TYPE and CLASSIFICATION After Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "encoder = OneHotEncoder(sparse=False, dtype=np.int64)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encoded_df = pd.DataFrame(encoder.fit_transform(model_df[[\"APPLICATION_TYPE\",\"CLASSIFICATION\"]]))\n",
    "\n",
    "# Add the encoded variable names to the DataFrame\n",
    "encoded_df.columns = encoder.get_feature_names([\"APPLICATION_TYPE\",\"CLASSIFICATION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "model_df.drop([\"APPLICATION_TYPE\",\"CLASSIFICATION\"],1, inplace=True)\n",
    "model_df = model_df.merge(encoded_df,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one hot encoding for features APPLICATION_TYPE and CLASSIFICATION\n",
    "for column_name in model_df.columns:\n",
    "    if column_name[0:16] == \"APPLICATION_TYPE\" or column_name[0:14] == \"CLASSIFICATION\":\n",
    "        print(f\"The column:[{column_name}] has [{len(model_df[column_name].unique())}] values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale ASK_AMT feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_ASK_AMT_scaler = scaler.fit(model_df[\"ASK_AMT\"].values.reshape(-1,1))\n",
    "\n",
    "# Scale the data\n",
    "model_df[\"ASK_AMT\"] = X_ASK_AMT_scaler.transform(model_df[\"ASK_AMT\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = model_df[\"IS_SUCCESSFUL\"].values\n",
    "X = model_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net\n",
    "number_input_features = len(X_train[0])\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add( tf.keras.layers.Dense(units=hidden_layer_1_nodes, input_dim=number_input_features, activation=hidden_layer_1_act_func))\n",
    "\n",
    "# Second hidden layer add if node count > 0\n",
    "if hidden_layer_2_nodes > 0:\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_layer_2_nodes, activation=hidden_layer_2_act_func))\n",
    "\n",
    "# Third hidden layer add if node count > 1\n",
    "if hidden_layer_3_nodes > 0:\n",
    "    nn.add(tf.keras.layers.Dense(units=hidden_layer_3_nodes, activation=hidden_layer_3_act_func))\n",
    "    \n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=output_layer_nodes, activation=output_layer_act_func))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train,y_train,epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and Error Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame containing training history\n",
    "history_df = pd.DataFrame(fit_model.history, index=range(1,len(fit_model.history[\"loss\"])+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss\n",
    "print(\"Model loss grapth\")\n",
    "history_df.plot(y=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy\n",
    "print(\"Model accuracy grapth\")\n",
    "history_df.plot(y=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test,y_test,verbose=2)\n",
    "print(f\"Test set statistics:  Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model if Accuracy >= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "if model_accuracy >= 0.75:\n",
    "    nn.save(\"charity_analysis_trained.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
